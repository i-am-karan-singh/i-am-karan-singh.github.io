<!DOCTYPE html>
<html>
  <head>
    <link rel="stylesheet" type="text/css" href="style/style.css">
    <link rel="icon" type="image/png" href="images/pu.png">
    <title>Karan Singh</title>
    <script src="js/scramble.js"></script>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
    </script>
    <script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-57818814-1', 'auto');
	  ga('send', 'pageview');
    </script>
    <script type="text/javascript" async
     src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  </head>
  <body>
  <table cellspacing="0" cellpadding="0" border="0" width="800" align="center">
    <tbody><tr>
    <td>
      <table cellspacing="0" cellpadding="10" border="0" width="80%" align="center">
      <tbody><tr bgcolor="#ffffff">
        <td width="100%" valign="middle" style="padding-left: 0px;">
        <p align="center">
          <name>Karan Singh</name><br/>
          <b>email</b>:
          <font id="covermail"><font id="email" style="display:inline;">
            <noscript><i>Please enable Javascript to view.</i></noscript>
          </font></font>
          <script>
          emailScramble = new scrambledString(document.getElementById('email'),
              'emailScramble', 'ranasduke@pincer.not',
              [3,4,5,2,6,20,21,1,19,7,8,10,11,12,13,9,17,16,15,14]);
          function unscram(){
              emailScramble.initAnimatedBubbleSort();
              var elem = document.getElementById('covermail');
              elem.style.backgroundColor = 'yellow';
          }
          </script>
        </p>
        <p>
          I'm a PhD candidate in <a href="http://cs.princeton.edu">Computer Science</a> at <a href="http://www.princeton.edu">Princeton University</a>. I'm very fortunate to be advised by <a href="http://cs.princeton.edu/~ehazan">Elad Hazan</a>. My research is focused on provably efficient algorithms for <strong>reinforcement learning</strong>, aided by the lens of <strong>dynamical systems</strong> and the algorithmic toolkit of <strong>optimization</strong> and (non-stochastic) <strong>online learning</strong>. My recent efforts seek to address the challenges that accompany sparse rewards, partial observability and large (or continuous) state spaces. At this time, I'm a long-term research intern at <a href="https://ai.google/research/">Google AI</a>.
        </p>
        <p>
          I did my bachelors at <a href="http://iitk.ac.in">IIT Kanpur</a>, where I worked on sketch-based algorithms for machine learning, and space lower bounds in the streaming model. I've also spent time at <a href="http://research.microsoft.com/en-us/">Microsoft Research</a> in Redmond working on program synthesis.
        </p>
        <p align="center">
          <a onclick="unscram();">Email</a> &nbsp;|&nbsp;
          <a href="https://i-am-karan-singh.github.io/docs/cv/KaranSingh-CV-full.pdf">CV</a> &nbsp;|&nbsp;
          <a href="https://scholar.google.com/citations?user=PZJIgZUAAAAJ&hl=en">Google Scholar</a> &nbsp;|&nbsp; 
          <a href="https://github.com/i-am-karan-singh/">Github</a>
        </p>
        </td>
        <td width="0%">
          <img width=150 src="images/karan.gif">
        </td>
      </tr>
      </tbody></table>
      
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr><td>
        <heading>News</heading>
        <ul>
          <li> <i>[December 2018]</i> Posted a new <a href="https://arxiv.org/abs/1812.02690">preprint</a> on <a href="#max-ent">maximum entropy exploration</a> &mdash; a provably efficient algorithm for exploring MDPs in the absence of a reward function.</li>
	        <li> <i>[September 2018]</i> Paper on <a href="#lds-asym">learning asymmetric LDS</a> accepted at <a href="https://nips.cc/Conferences/2018">NIPS 2018</a> (<strong>Oral Presentation</strong>).</li>
        </ul>
      </td></tr>

      <table cellspacing="0" cellpadding="20" border="0" width="100%" align="center">
      <tbody><tr>
        <td width="100%" valign="middle">
          <heading>Publications</heading>
        </td>
      </tr>
      </tbody></table>
  
  <table cellspacing="0" cellpadding="7" border="0" width="100%" align="center">
    <tbody>
      <tr bgcolor="#ffffd0">
          <td width="25%" valign="middle">
            <img width=220 src="images/max-ent.gif">
          </td>
          <td width="75%" valign="top">
            <p><a href="#" name="max-ent">
            <papertitle><img src="images/new.png" height="15">Provably Efficient Maximum Entropy Exploration</papertitle></a><br>
            with <a href="http://cs.princeton.edu/~ehazan">Elad Hazan</a>, <a href="https://homes.cs.washington.edu/~sham/">Sham Kakade</a>, <a href="http://www.abbyvansoest.com/">Abby Van Soest</a> <br>
            <em>Preprint</em>, 2018<br> 
            <a href="https://arxiv.org/pdf/1812.02690.pdf">pdf</a> | <a href="docs/max-ent/cite.bib">bibtex</a> | <a href="https://arxiv.org/abs/1812.02690">arXiv</a>
            </p><p></p>
      <p>Suppose an agent is in an unknown MDP in the absence of a reward signal, what might we hope that an agent can efficiently learn to do? One natural objective problem is to learn a policy which induces a distribution over state space that is as uniform as possible (in an entropic sense). Despite this mathematical program being non-convex, we provide a provably efficient method, both in terms of sample size and computational complexity, to construct such a maximum-entropy exploratory policy.</p>
          </td>
        </tr>
    <tr bgcolor="#ffffd0">
      <td width="25%" valign="middle">
        <img width=220 src="images/lds-new.png">
      </td>
      <td width="75%" valign="top">
        <p><a href="#" name="lds-news">
        <papertitle>Spectral Filtering for General Linear Dynamical Systems</papertitle></a><br>
        with <a href="http://cs.princeton.edu/~ehazan">Elad Hazan</a>, <a href="http://holdenlee.github.io/">Holden Lee</a>, <a href="http://cs.princeton.edu/">Cyril Zhang</a>, <a href="http://www.cs.princeton.edu/~yz7/">Yi Zhang</a>  <br>
        <em>Neural Information Processing Systems (NIPS)</em>, 2018 <strong>Oral Presentation</strong><br> 
        <a href="https://papers.nips.cc/paper/7714-spectral-filtering-for-general-linear-dynamical-systems.pdf">pdf</a> | <a href="docs/lds-new/cite.bib">bibtex</a> | <a href="https://arxiv.org/abs/1802.03981">arXiv</a>
        </p><p></p>
	<p>We give a polynomial-time algorithm for learning latent-state linear dynamical systems without system identification, and without assumptions on the spectral radius of the system's transition matrix. The algorithm extends the recently introduced technique of spectral filtering, previously applied only to systems with a symmetric transition matrix, using a novel convex relaxation to allow for the efficient identification of phases.</p>
      </td>
    </tr>
    <td width="25%" valign="middle">
        <img width=220 height=170 src="images/ggt.PNG">
      </td>
      <td width="75%" valign="top">
        <p><a href="#" name="ggt">
        <papertitle>The Case for Full-Matrix Adaptive Regularization</papertitle></a><br>
        with <a href="http://cs.princeton.edu/~namana/">Naman Agarwal</a>, <a href="https://www.cs.princeton.edu/~bbullins/">Brian Bullins</a>, Xinyi Chen, <a href="http://cs.princeton.edu/~ehazan">Elad Hazan</a>, <a href="http://cs.princeton.edu/">Cyril Zhang</a>, <a href="http://www.cs.princeton.edu/~yz7/">Yi Zhang</a> <br>
        Prelim at <em>ICML 2018 Workshop on Modern Trends in Non-Convex Optimization</em><br> 
        <a href="https://arxiv.org/pdf/1806.02958.pdf">pdf</a>
        |
        <a href="docs/ggt/cite.bib">bibtex</a>
        |
        <a href="https://arxiv.org/abs/1806.02958">arXiv</a>
        </p><p></p>
        <p>Adaptive regularization methods come in diagonal and full-matrix variants. However, only the former have enjoyed widespread adoption in training large-scale deep models. In this paper, we show how to make full-matrix adaptive regularization practical and useful. At the heart of our algorithm is an efficient method for computing the inverse square root of a low-rank matrix. We show that GGT converges to first-order local minima, providing the first rigorous theoretical analysis of adaptive regularization in non-convex optimization.</p>
      </td>
    </tr>
    <td width="25%" valign="middle">
        <img width=220 src="images/cartpole.jpg">
      </td>
      <td width="75%" valign="top">
        <p><a href="#" name="lds-rl">
        <papertitle>Towards Provable Control for Unknown Linear Dynamical Systems</papertitle></a><br>
        with Sanjeev Arora, <a href="http://cs.princeton.edu/~ehazan">Elad Hazan</a>, Holden Lee, Cyril Zhang, <a href="http://www.cs.princeton.edu/~yz7/">Yi Zhang</a> <br>
        Prelim at <em>ICLR 2018 Workshop</em><br> 
        <a href="https://openreview.net/pdf?id=BygpQlbA-">pdf</a>
        |
        <a href="docs/lqr-rl/cite.bib">bibtex</a>
        </p><p></p>
        <p>We study the control of symmetric linear dynamical systems with unknown dynamics and a hidden state. Using a recent spectral filtering technique, we formulate optimal control in this setting as a convex program. This approach eliminates the need to solve the nonconvex problem of explicit identification of the system and its latent state, and allows for provable optimality guarantees for the control signal.</p>
      </td>
    </tr>
    <tr>
      <td width="25%" valign="middle">
        <img width=220 src="images/lds.png">
      </td>
      <td width="75%" valign="top">
        <p><a href="#" name="lds-lr">
        <papertitle>Learning Linear Dynamical Systems
via Spectral Filtering</papertitle></a><br>
        with <a href="http://cs.princeton.edu/~ehazan">Elad Hazan</a>, <a href="http://cs.princeton.edu/">Cyril Zhang</a> <br>
        <em>Neural Information Processing Systems (NIPS)</em>, 2017 <strong>Spotlight</strong><br> 
        <a href="https://papers.nips.cc/paper/7247-learning-linear-dynamical-systems-via-spectral-filtering.pdf">pdf</a>
        |
        <a href="docs/lds-old/cite.bib">bibtex</a>
        |
        <a href="https://arxiv.org/abs/1711.00946">arXiv</a>
        </p><p></p>
        <p>We present an efficient and practical algorithm for the online prediction of discrete-time linear dynamical systems. Despite the non-convex optimization problem, our algorithm comes with provable guarantees: it has near-optimal regret bounds compared to the best LDS in hindsight, while overparameterizing the model by a small logarithmic factor. Our analysis brings together ideas from improper learning through convex relaxations, online regret minimization, and the spectral theory of Hankel matrices.</p>
      </td>
    </tr>
    <tr>
      <td width="25%" valign="middle">
        <img width=220 height=120 src="images/dp-price.png">
      </td>
      <td width="75%" valign="top">
        <p><a href="http://proceedings.mlr.press/v70/agarwal17a/agarwal17a.pdf" name="dp-price">
        <papertitle>The Price of Differential Privacy for Online Learning</papertitle></a><br>
		with <a href="http://cs.princeton.edu/~namana/">Naman Agarwal</a><br/>
        <em>International Conference on Machine Learning (ICML)</em>, 2017 <br> 
        <a href="http://proceedings.mlr.press/v70/agarwal17a/agarwal17a.pdf">pdf</a>
        |
        <a href="docs/dp-price/cite.bib">bibtex</a>
        |
        <a href="https://arxiv.org/abs/1701.07953">arXiv</a>
        </p><p></p>
        <p>We design differentially private algorithms for the problem of online linear optimization in the full information and bandit settings with optimal $O(\sqrt{T})$
regret bounds. In the full-information setting, our results demonstrate that $\epsilon$-differential privacy may be ensured for free. In particular, the regret bounds scale as $O\left(\sqrt{T}+{\epsilon}^{-1}\right)$. In the bandit setting, this is the first $O\left({\epsilon}^{-1}\sqrt{T}\right)$-regret algorithm.</p>
      </td>
    </tr>
	
    <tr>
      <td width="25%">
        <img width=220 src="images/onco-regret.png">
      </td>
      <td width="75%" valign="top">
        <p><a href="http://proceedings.mlr.press/v70/hazan17a/hazan17a.pdf" name="onco-regret">
        <papertitle>Efficient Regret Minimization in Non-Convex Games</papertitle></a><br>
          with <a href="http://cs.princeton.edu/~ehazan">Elad Hazan</a>, <a href="http://cs.princeton.edu/">Cyril Zhang</a> <br>
          <em>International Conference on Machine Learning (ICML)</em>, 2017 <br>        
          <a href="http://proceedings.mlr.press/v70/hazan17a/hazan17a.pdf">pdf</a>
        |
        <a href="docs/onco-regret/cite.bib">bibtex</a> |
        <a href"https://arxiv.org/abs/1708.00075">arXiv</a>
        </p><p></p>
        <p>We consider regret minimization in repeated games with non-convex loss functions. Minimizing the standard notion of regret is computationally intractable. Thus, we define a natural notion of regret which permits efficient optimization and generalizes offline guarantees for convergence to an approximate local optimum. We give gradient-based methods that achieve optimal regret, which in turn guarantee convergence to equilibrium in this framework.</p>
      </td>
    </tr>

    <tr>
      <td width="25%">
          <img width=220 src="images/ds-vae.png">
      </td>
      <td width="75%" valign="top">
        <p><a href="#">
          <papertitle>Dynamic Task Allocation for Crowdsourcing</a></papertitle><br>
          with <a href="http://www.pacm.princeton.edu/node/368">Irineo Cabreros</a>, <a href="http://www.orie.cornell.edu/people/students_phd.cfm?grad=2362&year=2017">Angela Zhou</a> <br>
          Prelim at <em>ICML Workshop on Data Efficient Machine Learning</em>, 2016<br>
        </td>
      </tr>
     </tbody></table>
      <table cellspacing="0" cellpadding="20" border="0" width="100%" align="center">
      <tbody><tr>
        <td>
        <heading>Teaching</heading> (Assistantships in Instruction)
        </td>
      </tr>
      </tbody></table>
      <table cellpadding="7" border="0" width="100%" align="center">
      <tbody><tr>
        <td width="25%"><img src="images/teach.png" width="220" height="160"></td>
        <td width="75%" valign="center">
        <p>
          <a href="http://www.cs.princeton.edu/courses/archive/fall16/cos402/">
          <papertitle>COS 402: Machine Learning and Artificial Intelligence</papertitle>
          </a><br>
          <em>Princeton University</em>, Fall 2016<br>
          <strong>Instructors</strong>: Prof. <a href="http://cs.princeton.edu/~arora/">Sanjeev Arora</a>, Prof. <a href="http://cs.princeton.edu/~ehazan">Elad Hazan</a><br>
        </p>
        <p>
          <a href="http://www.cs.princeton.edu/courses/archive/fall17/cos324/">
          <papertitle>COS 324: Introduction to Machine Learning</papertitle>
          </a><br>
          <em>Princeton University</em>, Fall 2017<br>
          <strong>Instructors</strong>: Prof. <a href="http://cs.princeton.edu/~ehazan">Elad Hazan</a>, Prof. <a href="http://cs.princeton.edu/~ysinger">Yoram Singer</a>
          <br>
        </p>
        </td>
      </tr>
      </tbody></table>
      <table cellspacing="0" cellpadding="20" border="0" width="100%" align="center">
      <tbody><tr>
        <td>
        <br>
        <p align="right">
          <font size="1">
          A minimal adaptation of
          <a href="https://people.eecs.berkeley.edu/~pathak/">this</a>, and hence
          <a href="https://jonbarron.info/">this</a>
          and
          <a href="http://jeffdonahue.com/">this</a>.
	    </font>
        </p>
        </td>
      </tr>
      </tbody></table>
    </td>
    </tr>
  </tbody></table>
  

</body></html>
